{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "not terminated       0.93      0.97      0.95       607\n",
      "    terminated       0.92      0.83      0.87       263\n",
      "\n",
      "      accuracy                           0.93       870\n",
      "     macro avg       0.93      0.90      0.91       870\n",
      "  weighted avg       0.93      0.93      0.93       870\n",
      "\n",
      "============================================================\n",
      "Model: Decision Tree\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "not terminated       0.96      0.95      0.96       607\n",
      "    terminated       0.89      0.90      0.90       263\n",
      "\n",
      "      accuracy                           0.94       870\n",
      "     macro avg       0.92      0.93      0.93       870\n",
      "  weighted avg       0.94      0.94      0.94       870\n",
      "\n",
      "============================================================\n",
      "Model: Random Forest\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "not terminated       0.96      0.96      0.96       607\n",
      "    terminated       0.91      0.90      0.90       263\n",
      "\n",
      "      accuracy                           0.94       870\n",
      "     macro avg       0.93      0.93      0.93       870\n",
      "  weighted avg       0.94      0.94      0.94       870\n",
      "\n",
      "============================================================\n",
      "Model: Naive Bayes\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "not terminated       0.90      0.89      0.89       607\n",
      "    terminated       0.74      0.76      0.75       263\n",
      "\n",
      "      accuracy                           0.85       870\n",
      "     macro avg       0.82      0.82      0.82       870\n",
      "  weighted avg       0.85      0.85      0.85       870\n",
      "\n",
      "============================================================\n",
      "Model: Support Vector Machine\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "not terminated       0.75      0.85      0.80       607\n",
      "    terminated       0.50      0.36      0.42       263\n",
      "\n",
      "      accuracy                           0.70       870\n",
      "     macro avg       0.63      0.60      0.61       870\n",
      "  weighted avg       0.68      0.70      0.68       870\n",
      "\n",
      "============================================================\n",
      "Model: K-Nearest Neighbors\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "not terminated       0.95      0.98      0.96       607\n",
      "    terminated       0.94      0.87      0.91       263\n",
      "\n",
      "      accuracy                           0.94       870\n",
      "     macro avg       0.94      0.92      0.93       870\n",
      "  weighted avg       0.94      0.94      0.94       870\n",
      "\n",
      "============================================================\n",
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "1080 fits failed out of a total of 4320.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "416 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "664 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\HP\\deploy\\deploy-model-1\\venv1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.93758951 0.93039816 0.94046639 0.93873894 0.9424816  0.93557471\n",
      " 0.93902754 0.93471264 0.94161705 0.93068304 0.94104275 0.93298644\n",
      " 0.94161746 0.92752295 0.94161746 0.92752295 0.94334202 0.93269826\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.89704333 0.87345613 0.90307161 0.87230588 0.89760357 0.87086827\n",
      " 0.84066071 0.83060366 0.82828868 0.82714546 0.83776896 0.82572191\n",
      " 0.8176598  0.79149301 0.8176598  0.79149301 0.81506988 0.78373977\n",
      " 0.88149797 0.8665542  0.87431572 0.87143926 0.87748077 0.84641983\n",
      " 0.72993798 0.75580915 0.73798396 0.73538783 0.75467336 0.73711734\n",
      " 0.71958447 0.69485653 0.71958447 0.69485653 0.73080542 0.69399322\n",
      " 0.92206359 0.90796701 0.9229269  0.90796701 0.92379062 0.90767965\n",
      " 0.92465228 0.90854296 0.92436451 0.90825519 0.92522823 0.9082556\n",
      " 0.92407508 0.90308195 0.92407508 0.90308195 0.92378731 0.90164517\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.77511412 0.75497271 0.77223642 0.7584214  0.77828    0.75957248\n",
      " 0.74835649 0.75813859 0.74461548 0.75957496 0.74979534 0.7572728\n",
      " 0.75727776 0.75755933 0.75727776 0.75755933 0.75008352 0.75698462\n",
      " 0.72908377 0.70175556 0.71412015 0.70089184 0.71958778 0.70089184\n",
      " 0.71757091 0.70232697 0.726204   0.70232697 0.71900976 0.70232697\n",
      " 0.71526958 0.69312991 0.71526958 0.69312991 0.71526958 0.69312991\n",
      " 0.93845489 0.93298851 0.94334326 0.93643554 0.94420615 0.93471306\n",
      " 0.94161664 0.93212313 0.94276689 0.93096998 0.94305673 0.93327297\n",
      " 0.94219176 0.92838832 0.94219176 0.92838832 0.94276689 0.93011122\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.85160506 0.8004238  0.85705491 0.80358596 0.84871868 0.81221657\n",
      " 0.84757587 0.78315348 0.82455635 0.79063425 0.81390308 0.79467295\n",
      " 0.83290416 0.77712065 0.83290416 0.77712065 0.82887538 0.78633052\n",
      " 0.7774109  0.7374171  0.75900066 0.74432316 0.75612462 0.73482924\n",
      " 0.74921773 0.73251302 0.73281485 0.72705325 0.73655172 0.73424543\n",
      " 0.71900976 0.69485653 0.71900976 0.69485653 0.72879393 0.69485653\n",
      " 0.93874101 0.92809477 0.94248077 0.93500124 0.94017944 0.93327752\n",
      " 0.93931489 0.9335628  0.94104151 0.93298602 0.94104275 0.93384933\n",
      " 0.94161746 0.92752295 0.94161746 0.92752295 0.94334202 0.93269826\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.86942115 0.84614322 0.87517986 0.83636236 0.87374225 0.83752171\n",
      " 0.8363545  0.80586868 0.81563921 0.81276565 0.82196477 0.82831101\n",
      " 0.8176598  0.7935074  0.8176598  0.7935074  0.81650872 0.7958261\n",
      " 0.79063549 0.76790333 0.79668155 0.75495659 0.8122112  0.75811627\n",
      " 0.72993798 0.74803936 0.73712189 0.73740222 0.75467336 0.74344828\n",
      " 0.71699826 0.69485653 0.71699826 0.69485653 0.72879393 0.69399322\n",
      " 0.93758951 0.93039816 0.94046639 0.93873894 0.9424816  0.93557471\n",
      " 0.93902754 0.93471264 0.94161705 0.93068304 0.94104275 0.93298644\n",
      " 0.94161746 0.92752295 0.94161746 0.92752295 0.94334202 0.93269826\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.88841355 0.87260192 0.87633672 0.86166832 0.90163896 0.85735715\n",
      " 0.83749979 0.8340602  0.82886339 0.82858927 0.83776896 0.8314773\n",
      " 0.8176598  0.79149301 0.8176598  0.79149301 0.81506988 0.78373977\n",
      " 0.83176879 0.79666212 0.82659183 0.8018366  0.80672662 0.80701315\n",
      " 0.72993798 0.75580915 0.73798396 0.73538783 0.75467336 0.73711734\n",
      " 0.71929711 0.69485653 0.71929711 0.69485653 0.73195485 0.69399322\n",
      " 0.93758951 0.93039816 0.94046639 0.93873894 0.9424816  0.93557471\n",
      " 0.93902754 0.93471264 0.94161705 0.93068304 0.94104275 0.93298644\n",
      " 0.94161746 0.92752295 0.94161746 0.92752295 0.94334202 0.93269826\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.89301745 0.88151079 0.89645291 0.86368436 0.90192177 0.8622389\n",
      " 0.84066071 0.83060779 0.82828868 0.82628339 0.83776896 0.82744728\n",
      " 0.8176598  0.79149301 0.8176598  0.79149301 0.81506988 0.78373977\n",
      " 0.87086124 0.8265786  0.84383693 0.81334532 0.84124411 0.82457\n",
      " 0.72993798 0.75580915 0.73798396 0.73538783 0.75467336 0.73711734\n",
      " 0.71958447 0.69485653 0.71958447 0.69485653 0.73080542 0.69399322\n",
      " 0.93586042 0.93212354 0.93988795 0.9381634  0.94161292 0.93931737\n",
      " 0.9347081  0.93327628 0.93470768 0.93327586 0.93902299 0.931548\n",
      " 0.94046018 0.93384933 0.94046018 0.93384933 0.94074795 0.93413917\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.89876003 0.88525345 0.90422683 0.87288803 0.89186554 0.86772017\n",
      " 0.85044819 0.84240511 0.84497974 0.83118292 0.83462292 0.83866865\n",
      " 0.8113206  0.81593815 0.8113206  0.81593815 0.81853014 0.81766104\n",
      " 0.87546804 0.8656909  0.88121269 0.87575994 0.88667494 0.86138096\n",
      " 0.72734971 0.72734681 0.74490449 0.72734681 0.73166873 0.73482552\n",
      " 0.7215939  0.69370545 0.7215939  0.69370545 0.72908005 0.69370545\n",
      " 0.90883156 0.89789217 0.9088299  0.88668321 0.91026875 0.89416563\n",
      " 0.90940544 0.89674109 0.90940544 0.88898536 0.91055569 0.89215083\n",
      " 0.90969156 0.90479658 0.90969156 0.90479658 0.90940379 0.90508435\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.75151493 0.76361035 0.7509398  0.7647573  0.74863764 0.76274415\n",
      " 0.74288721 0.76360953 0.74375052 0.76447035 0.74375052 0.76533366\n",
      " 0.72706028 0.75929629 0.72706028 0.75929629 0.72734805 0.76245803\n",
      " 0.71728521 0.70491689 0.71239353 0.70376499 0.71728562 0.7046283\n",
      " 0.7167076  0.70232697 0.71498098 0.70232697 0.71814645 0.70261474\n",
      " 0.71526958 0.69312991 0.71526958 0.69312991 0.71526958 0.69312991\n",
      " 0.93730009 0.93499959 0.94075167 0.93730092 0.94247705 0.93701356\n",
      " 0.93729926 0.93557719 0.93816216 0.93557471 0.94161416 0.93183536\n",
      " 0.94103779 0.93816216 0.94103779 0.93816216 0.94132556 0.93270032\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.82255272 0.82976681 0.84383073 0.83495038 0.82571488 0.81222525\n",
      " 0.83290002 0.80243405 0.82225089 0.80847929 0.8087377  0.80963161\n",
      " 0.8024109  0.79208302 0.8024109  0.79208302 0.82226991 0.79553626\n",
      " 0.77310014 0.74172621 0.75814066 0.73108203 0.75612544 0.73367692\n",
      " 0.7256297  0.72618912 0.72821508 0.7210167  0.73224427 0.72878442\n",
      " 0.71757091 0.69428099 0.71757091 0.69428099 0.72879269 0.69428099\n",
      " 0.93729885 0.93183577 0.93844952 0.93959935 0.94190027 0.94161374\n",
      " 0.93499462 0.93471099 0.93442033 0.93385016 0.93815968 0.93413462\n",
      " 0.94017283 0.9318366  0.94017283 0.9318366  0.94103531 0.93270198\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.88928099 0.85908914 0.89386835 0.86080915 0.88035806 0.86023443\n",
      " 0.84844249 0.83607211 0.83290292 0.83463657 0.8283015  0.83262011\n",
      " 0.8113206  0.81363888 0.8113206  0.81363888 0.81853014 0.81910651\n",
      " 0.77913834 0.77738774 0.79495162 0.77134293 0.79351567 0.7793951\n",
      " 0.72188994 0.72734681 0.73800794 0.72734681 0.72592161 0.73482552\n",
      " 0.71584677 0.69370545 0.71584677 0.69370545 0.72390763 0.69370545\n",
      " 0.93614777 0.93241131 0.93988795 0.93672455 0.94161292 0.93931737\n",
      " 0.9347081  0.93385182 0.93470768 0.93614943 0.93902299 0.931548\n",
      " 0.94046018 0.93384933 0.94046018 0.93384933 0.94074795 0.93413917\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.88467088 0.86196436 0.89244108 0.86541594 0.90163194 0.87058422\n",
      " 0.85908542 0.83808856 0.83749731 0.83262176 0.83606136 0.83751923\n",
      " 0.8113206  0.81593815 0.8113206  0.81593815 0.81853014 0.81766104\n",
      " 0.82055445 0.8027057  0.82054536 0.79752584 0.8262929  0.79378773\n",
      " 0.72706235 0.72734681 0.74260564 0.72734681 0.73224345 0.73482552\n",
      " 0.72015712 0.69370545 0.72015712 0.69370545 0.73022947 0.69370545\n",
      " 0.93586042 0.93183577 0.93988795 0.9381634  0.94161292 0.93931737\n",
      " 0.9347081  0.93327628 0.93470768 0.93327586 0.93902299 0.931548\n",
      " 0.94046018 0.93384933 0.94046018 0.93384933 0.94074795 0.93413917\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.89760647 0.87835277 0.90595675 0.87892334 0.90049367 0.8749049\n",
      " 0.85044819 0.84240511 0.84497974 0.83118292 0.83491028 0.83866865\n",
      " 0.8113206  0.81593815 0.8113206  0.81593815 0.81853014 0.81766104\n",
      " 0.84126685 0.82457289 0.84039196 0.82715207 0.83290788 0.83001571\n",
      " 0.72734971 0.72734681 0.74490449 0.72734681 0.73166873 0.73482552\n",
      " 0.7215939  0.69370545 0.7215939  0.69370545 0.72908005 0.69370545]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Decision Tree:  {'classifier__criterion': 'gini', 'classifier__max_depth': 20, 'classifier__max_features': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 10, 'classifier__splitter': 'best'}\n",
      "Best cross-validation accuracy for Decision Tree: 0.94\n",
      "Tuned Decision Tree Model:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "not terminated       0.96      0.96      0.96       607\n",
      "    terminated       0.90      0.90      0.90       263\n",
      "\n",
      "      accuracy                           0.94       870\n",
      "     macro avg       0.93      0.93      0.93       870\n",
      "  weighted avg       0.94      0.94      0.94       870\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "# Load data\n",
    "data = pd.read_excel(r'C:\\\\Users\\\\HP\\\\Desktop\\\\final project12.xlsx')\n",
    "# Drop rows with missing target values\n",
    "data.dropna(subset=['Job status'], inplace=True)\n",
    "# Define features and target\n",
    "X = data.drop(columns=['Job status', 'No'])\n",
    "y = data['Job status']\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['Sex', 'Label', 'Status', 'College', 'Fild of Study']\n",
    "numerical_cols = ['Salary', 'age', 'year_of_service']\n",
    "# Preprocessing for numerical data: impute missing values with mean\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "# Preprocessing for categorical data: impute missing values and one-hot encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "# Function to convert sparse matrix to dense\n",
    "def to_dense(X):\n",
    "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "\n",
    "dense_transformer = FunctionTransformer(to_dense)\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=10000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Naive Bayes': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('to_dense', dense_transformer),\n",
    "        ('classifier', GaussianNB())\n",
    "    ]),\n",
    "    'Support Vector Machine': SVC(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    if name == 'Naive Bayes':\n",
    "        clf = model\n",
    "    else:\n",
    "        clf = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "    \n",
    "    # Train the model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate the model\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"=\"*60)\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "pipeline_dt = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', dt)])\n",
    "\n",
    "param_grid_dt = {\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "    'classifier__splitter': ['best', 'random'],\n",
    "    'classifier__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': [None, 'auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(estimator=pipeline_dt, param_grid=param_grid_dt, \n",
    "                              cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "best_params_dt = grid_search_dt.best_params_\n",
    "best_score_dt = grid_search_dt.best_score_\n",
    "\n",
    "print(\"Best parameters for Decision Tree: \", best_params_dt)\n",
    "print(\"Best cross-validation accuracy for Decision Tree: {:.2f}\".format(best_score_dt))\n",
    "\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "print(\"Tuned Decision Tree Model:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_decision_tree_model.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_dt, 'best_decision_tree_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
